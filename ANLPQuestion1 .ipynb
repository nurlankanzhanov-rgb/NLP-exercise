{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applied Natural Language Processing 955G5\n",
        "## Computer Based Examination, 2023\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ],
      "metadata": {
        "id": "BThZppujhxPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 260893"
      ],
      "metadata": {
        "id": "9XarARfLiFmE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 (50 marks)\n",
        "\n",
        "This Question is about Question Answering."
      ],
      "metadata": {
        "id": "Hqv3rUCwgi2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-apneMCEgh_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdd1ae0-9c04-4a9c-b730-18624a64739d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentences=[\"Sheikh Mohamed bin Zayed Al Nahyan GCMG, colloquially known by his initials as MBZ, is the third president of the United Arab Emirates and the ruler of Abu Dhabi. \",\n",
        "           \"On April 13, 2007, Google reached an agreement to acquire DoubleClick for $3.1 billion, transferring to Google valuable relationships that DoubleClick had with Web publishers and advertising agencies.\",\n",
        "           \"Sheikh Mohamed bin Zayed Al Nahyan GCMG was born on the 11th of March 1961.\",\n",
        "           \"Ouagadougou is the capital of Burkina Faso and the administrative, communications, cultural, and economic centre of the nation.\",\n",
        "           \"President Volodymyr Zelensky had a conversation with the president of the United Arab Emirates and thanked him for supporting the sovereignty and territorial integrity of Ukraine.\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) For each of the following questions, identify the answer type (i.e. in terms of a named entity tag) and the features of the question a system might use to predict these types."
      ],
      "metadata": {
        "id": "fM3NVu5ehq68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) \"Who is the president of the UAE?\" (2 marks)\n",
        "\n",
        "ii) \"Which company did Google buy?\" (2 marks)\n",
        "\n",
        "iii) \"Where is Ouagadougou?\" (2 marks)"
      ],
      "metadata": {
        "id": "xQ8iNZzMhDra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) it should be\"PERSON' NEG. The answer should be person. so, it is Sheikh Mohamed bin Zayed Al Nahyan\n",
        "ii) It should be \"Org\". Company name which bought: Doubleclick\n",
        "iii)it should 'GPE' NEG, Place. Burkina Faso"
      ],
      "metadata": {
        "id": "6EYQOQwA2dZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) By following the steps below, build an inverted index for the sentences above. This will be a dictionary whose keys are tokens and whose values are indexes into the list of sentences."
      ],
      "metadata": {
        "id": "z4Rr3uSFln8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Tokenize the sentences and call the result `tokenized_sentences`. In other words, produce a list of lists of tokens, by turning each string in `sentences` into a list of tokens. For example: `[\"This is an example\",\"This is another\"]` would become `[[\"This\",\"is\",\"an\",\"example\"],[\"This\",\"is\",\"another\"]]` (4 marks)"
      ],
      "metadata": {
        "id": "2Mtlq5jnmWWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "tokenized_sentences\n"
      ],
      "metadata": {
        "id": "9KaDiMqNmc-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Construct a dictionary called `inverted_index` that allows us to lookup the the sentences it occurs in. In other words, for each token in the lists within `tokenized_sentences`, build up a list of the indexes of the sentences it occurs in. For example, \"Sheikh\" occurs in sentences with indexes 0 and 2, so `inverted_index[\"Sheikh\"]` has the value `[0,2]` (10 marks)."
      ],
      "metadata": {
        "id": "HeVGWyK5mdpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "inverted_index = {}\n",
        "\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    for j, word in enumerate(sent):\n",
        "        indexes = set([i])\n",
        "        for k, lst in enumerate(tokenized_sentences):\n",
        "            if word in lst:\n",
        "                indexes.add(k)\n",
        "        inverted_index[word] = list(indexes)\n",
        "\n",
        "inverted_index\n"
      ],
      "metadata": {
        "id": "k88ysLZcGe9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d279a86-3dfb-4ea7-9dee-bf6e528e0100"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) An Information Retrieval system attempts to find relevant passages to answer the first question above about the president of the UAE. It retrieves two sentences based on relevant keywords, but only one of those sentences contains the actual answer to the question."
      ],
      "metadata": {
        "id": "fuRVaDy1sotr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "retrieved=[0,4]\n",
        "relevant =[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "ufNcqCm5ttQS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Write a function that returns the precision and recall of a retrieval system, given two inputs representing the sentences that were retrieved and the sentences that were actually relevant to the query. Each input will be a list of indices. Apply the function to the lists given above. (6 marks)"
      ],
      "metadata": {
        "id": "Kj_BptMUw0Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Why might it be difficult to retrieve the relevant sentence to answer the question about the president of the UAE from your inverted index? What additional processing might make this more effective? (4 marks)"
      ],
      "metadata": {
        "id": "o411W8cN1CKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "might use different words or phrases to describe the president of the UAE compared to those used in the documents like GCMB or MBZ. Also may not have enough data to recognize the president of the UAE. Using more data and when he is mentioning in context it should be the same each time, it should not mention like MBZ or GCMB always should be mentioned president of UEA"
      ],
      "metadata": {
        "id": "i4W3DUWP2a2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) What additional difficulties are involved in answering the question \"When was the president of the UAE born?\" from the set of sentences above? (5 marks)"
      ],
      "metadata": {
        "id": "JsjHtTzX2bRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ambiguity, Synonym, and other word classify problems."
      ],
      "metadata": {
        "id": "NURddwNk4N0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d)\n",
        "\n",
        "i) Describe at least two other forms of pre-processing that might be applied to the sentences above, and discuss the impact they might have on the question answering task. (5 marks)"
      ],
      "metadata": {
        "id": "1LyBQhb5AM82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization , Normalization, stemming\n"
      ],
      "metadata": {
        "id": "Ucjp4Q7bAMg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Explain how a Knowledge Based approach to Question Answering works and how it differs from the Information retrieval approach discussed so far. Give examples of systems that use a knowledge based approach. (10 marks)"
      ],
      "metadata": {
        "id": "Td4Mnoi54ORq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A knowledge-based approach to question answering involves using a large, pre-existing knowledge base to answer questions. This is in contrast to information retrieval approaches, which search through a large corpus of text to find the answer to a question.\n",
        "\n",
        "One way that a knowledge-based system might work is by representing the knowledge base as a graph, with concepts represented as nodes and relationships between concepts represented as edges. When a user asks a question, the system can search the graph for the relevant information and return an answer based on the connections between the concepts in the graph.\n",
        "\n",
        "One example of a system that uses a knowledge-based approach is IBM Watson. Watson is a question answering system that uses a combination of natural language processing, machine learning, and a large knowledge base to answer questions. Other examples of knowledge-based systems include CYC and OpenMind.\n",
        "\n",
        "Overall, knowledge-based approaches tend to be more effective at answering specific, well-defined questions, as they rely on a pre-existing knowledge base rather than searching through a large corpus of text. However, they may not be as effective at answering more open-ended or complex questions, or questions about information that is not contained in the knowledge base.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R3ZJrdaQAK8q"
      }
    }
  ]
}