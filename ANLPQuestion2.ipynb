{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applied Natural Language Processing 955G5\n",
        "## Computer Based Examination, 2023\n",
        "\n",
        "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
      ],
      "metadata": {
        "id": "0sTS7cMjiUHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update your candidate number here\n",
        "candidate_number = 260893"
      ],
      "metadata": {
        "id": "AXFCZsF0iUkz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 (50 marks)\n",
        "\n",
        "This Question is about POS Tagging.\n",
        "\n"
      ],
      "metadata": {
        "id": "grPE5uwpVI65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7KspiAfrVF_n"
      },
      "outputs": [],
      "source": [
        "### do not change the code in this cell\n",
        "# make sure you run this cell\n",
        "tagged_sentences=[\"john_N loves_V chocolate_N\",\n",
        "                  \"bob_N hates_V meetings_N\",\n",
        "                  \"alice_N likes_V fred_N\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) \n",
        "\n",
        "i) Use the `split` method to break each sentence in `tagged_sentences` into `word_tag` tokens. The result should be a list of lists and be named `tagged_words` For example, `[\"dogs_N bark_V\"]` should become `[[\"dogs_N\",\"bark_V\"]]`. (4 marks)\n",
        "\n"
      ],
      "metadata": {
        "id": "LgT47zd8WJNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words = [sentence.split() for sentence in tagged_sentences]\n",
        "for i in range(len(tagged_words)):\n",
        "  print(tagged_words[i])"
      ],
      "metadata": {
        "id": "CkoECVuwdxrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46e65c0-c9d1-4829-c7b3-d97e59acd8eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['john_N', 'loves_V', 'chocolate_N']\n",
            "['bob_N', 'hates_V', 'meetings_N']\n",
            "['alice_N', 'likes_V', 'fred_N']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Use the `split` method again to break each token in `tagged_words` into `(word,tag)` tuples. The result should be a list of lists of tuples and be named `words_and_tags` For example, `[[\"dogs_N\", \"bark_V\"]]` should become `[[(\"dogs\",\"N\"), (\"bark\",\"V\")]]`. (6 marks)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_0cSNormdyz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_and_tags = [[tuple(token.split(\"_\")) for token in sentence] for sentence in tagged_words]\n",
        "words_and_tags\n"
      ],
      "metadata": {
        "id": "36U7HvzfeyB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2751b7cb-9967-4878-b02c-4049a7c0c932"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('john', 'N'), ('loves', 'V'), ('chocolate', 'N')],\n",
              " [('bob', 'N'), ('hates', 'V'), ('meetings', 'N')],\n",
              " [('alice', 'N'), ('likes', 'V'), ('fred', 'N')]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Calculate the counts for all tags in `words_and_tags`. The result should be a dictionary called `tag_counts`. For example, the tag \"V\" occurs 3 times, so `tag_counts[\"V\"]` will be `3`. (4 marks)"
      ],
      "metadata": {
        "id": "RIuwCLsPeyvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "tags = [tag for sentence in words_and_tags for word, tag in sentence]\n",
        "tag_counts = Counter(tags)\n",
        "tag_counts[\"V\"]"
      ],
      "metadata": {
        "id": "f62Qza4XfbGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7741a50-c833-418e-c997-5eef088f1c7f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv) Calculate the tag counts for all words in `words_and_tags` and call the result `word_tag_counts`. This should be a dictionary of dictionaries with the outer dictionary having words as keys and the values being themselves dictionaries that contain the tag counts for that word. For example, `word_tag_counts[\"john\"][\"N\"]=1`. (6 marks)"
      ],
      "metadata": {
        "id": "RdPWLXiNfbjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag_counts = {}\n",
        "\n",
        "for sentence in words_and_tags:\n",
        "    for word, tag in sentence:\n",
        "        if word not in word_tag_counts:\n",
        "            word_tag_counts[word] = {}\n",
        "        if tag not in word_tag_counts[word]:\n",
        "            word_tag_counts[word][tag] = 0\n",
        "        word_tag_counts[word][tag] += 1\n",
        "\n",
        "word_tag_counts[\"john\"][\"N\"]"
      ],
      "metadata": {
        "id": "xQU64ejVg5_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328df5d7-b1c0-49d9-c0c8-432dbdca8a41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Name three other common part-of-speech classes, and an example of a word that is found in each class. (6 marks)"
      ],
      "metadata": {
        "id": "ILO3VZ6RhHZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjectives: Adjectives describe or modify nouns or pronouns. Examples of adjectives include \"happy\", \"red\", \"large\", etc.\n",
        "\n",
        "Adverbs: Adverbs describe or modify verbs, adjectives, or other adverbs. Examples of adverbs include \"quickly\", \"loudly\", \"happily\", etc.\n",
        "\n",
        "Pronouns: Pronouns are used in place of nouns or noun phrases. Examples of pronouns include \"he\", \"she\", \"it\", \"they\", \"whose\", etc."
      ],
      "metadata": {
        "id": "9m5dAMi1EMZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Construct a unigram tagger by following the steps below."
      ],
      "metadata": {
        "id": "Rrr2XXy5EM-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Calculate the tag probabilities p(t) from the `tag_counts` dictionary by dividing each tag count by the total count and put the result in a dictionary called `tag_probs`. (3 marks)"
      ],
      "metadata": {
        "id": "bIJklFHwEjNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tag_probs = {}\n",
        "\n",
        "total_count = sum(tag_counts.values())\n",
        "\n",
        "for tag, count in tag_counts.items():\n",
        "    tag_probs[tag] = count / total_count\n",
        "tag_probs"
      ],
      "metadata": {
        "id": "4GfOLhidFC41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44dc7e4-e2a6-4f1b-d139-b3a6640c3e46"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'N': 0.6666666666666666, 'V': 0.3333333333333333}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Calculate the emission probabilities p(w|t) from the `word_tag_counts` dictionary by dividing each word-tag count by the total count for each tag, and put the result in a dictionary called `word_tag_probs`. (5 marks)"
      ],
      "metadata": {
        "id": "UoB_NrryFDcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag_probs = {}\n",
        "\n",
        "for word, tag_counts in word_tag_counts.items():\n",
        "    total_count = sum(tag_counts.values())\n",
        "    word_tag_probs[word] = {}\n",
        "    for tag, count in tag_counts.items():\n",
        "        word_tag_probs[word][tag] = count / total_count\n",
        "word_tag_probs"
      ],
      "metadata": {
        "id": "PZM4k95cFhgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634d4731-01c9-49dd-a128-257ff7633f67"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'john': {'N': 1.0},\n",
              " 'loves': {'V': 1.0},\n",
              " 'chocolate': {'N': 1.0},\n",
              " 'bob': {'N': 1.0},\n",
              " 'hates': {'V': 1.0},\n",
              " 'meetings': {'N': 1.0},\n",
              " 'alice': {'N': 1.0},\n",
              " 'likes': {'V': 1.0},\n",
              " 'fred': {'N': 1.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Write a function that takes a list of words and outputs the most probable tag for each word. Apply your function to the list `[\"fred\",\"likes\",\"meetings\"]`. (10 marks)"
      ],
      "metadata": {
        "id": "J9h_nl-pIqeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_probable_tags(words, tag_probs, word_tag_probs):\n",
        "    tags = []\n",
        "    for word in words:\n",
        "        if word in word_tag_probs:\n",
        "            best_tag = max(word_tag_probs[word], key=word_tag_probs[word].get)\n",
        "        else:\n",
        "            best_tag = max(tag_probs, key=tag_probs.get)\n",
        "        tags.append(best_tag)\n",
        "    return tags    \n",
        "\n",
        "a=[\"fred\",\"meetings\",\"hates\"]\n",
        "most_probable_tags(a, tag_probs, word_tag_probs)\n"
      ],
      "metadata": {
        "id": "OQLnGU-ON5iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416e0f13-ce09-45d4-f7bc-a576cd12190d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N', 'N', 'V']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d)\n",
        "\n",
        "i) The unigram tagger defined above ignores the order of tags and words. A better approach is to assign probabilities to sequences of tags, rather than to each tag individually. Describe the Markov assumption as it applies in this case. (2 marks)"
      ],
      "metadata": {
        "id": "DY7JDPE-N-at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Markov assumption states that the probability of a certain event depends only on the probability of the previous event . This means that the probability of a tag can be calculated based on the probability of the previous tag, without taking into account any other previous tags.This is a simplification that can make it easier to calculate probabilities, but it may not always accurately reflect the true probability of a tag in a given context. For example,  with a bigram tagger that takes into account the previous tag, the probability of the tag \"V\" would be calculated based on the frequency of the sequence \"N V\" in the training data. This would allow the tagger to take into account the fact that the word \"loves\" is more likely to be tagged as \"V\" after a noun (such as \"John\") than after another verb."
      ],
      "metadata": {
        "id": "yBLgfoH7O3AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) In the case where we are trying to find the most probable sequence of tags, the simplest approach would be to calculate a probability for every possible assignment of tags to words. Why might we want to avoid doing this, and what algorithm can we use instead? (4 marks)"
      ],
      "metadata": {
        "id": "rfc8ne91O39T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t is generally not feasible to calculate a probability for every possible assignment of tags to words, because the number of possible assignments grows exponentially with the number of words. \n",
        "\n",
        "Instead of calculating the probability of every possible assignment of tags to words, it is common to use an algorithm called the Viterbi algorithm to find the most probable sequence of tags. The Viterbi algorithm is a dynamic programming algorithm that uses the Markov assumption to efficiently calculate the most probable sequence of tags by considering the probabilities of the previous tags and the probabilities of the transitions between tags.\n"
      ],
      "metadata": {
        "id": "K6VFkX4hS9ET"
      }
    }
  ]
}